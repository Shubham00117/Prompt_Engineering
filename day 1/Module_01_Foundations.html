<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Module 01: Foundations of Prompt Engineering</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, sans-serif;
            background: linear-gradient(135deg, #1a1a2e 0%, #16213e 50%, #0f3460 100%);
            min-height: 100vh;
            padding: 40px 20px;
            color: #333;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
        }

        h1 {
            text-align: center;
            color: #00d9ff;
            margin-bottom: 40px;
            font-size: 2.5em;
            text-shadow: 0 0 20px rgba(0, 217, 255, 0.5);
        }

        .section {
            background: rgba(255, 255, 255, 0.95);
            border-radius: 15px;
            padding: 40px;
            margin-bottom: 30px;
            box-shadow: 0 10px 40px rgba(0, 0, 0, 0.3);
        }

        .section h2 {
            color: white;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            padding: 20px;
            border-radius: 10px;
            margin: -40px -40px 30px -40px;
            text-align: center;
            font-size: 1.8em;
        }

        .section h3 {
            color: #667eea;
            margin-top: 30px;
            margin-bottom: 15px;
            font-size: 1.4em;
            border-bottom: 3px solid #667eea;
            padding-bottom: 10px;
        }

        .why-matters {
            background: linear-gradient(135deg, #ff6b6b15, #feca5715);
            border-left: 5px solid #ff6b6b;
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
            font-size: 1.1em;
            line-height: 1.8;
        }

        .explanation {
            background: #e3f2fd;
            border-left: 5px solid #2196f3;
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
            font-size: 1.1em;
            line-height: 1.8;
        }

        .example-box {
            background: #f0f7ff;
            border: 3px solid #667eea;
            border-radius: 12px;
            padding: 25px;
            margin: 25px 0;
        }

        .example-box h4 {
            color: #667eea;
            margin-bottom: 15px;
            font-size: 1.2em;
        }

        .key-concept {
            background: #e8f5e9;
            border-left: 5px solid #4caf50;
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
            font-size: 1.1em;
            line-height: 1.8;
        }

        .summary-box {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 25px;
            border-radius: 12px;
            margin: 25px 0;
            font-size: 1.2em;
            text-align: center;
            font-weight: bold;
        }

        .diagram {
            background: white;
            border: 2px solid #667eea;
            border-radius: 12px;
            padding: 30px;
            margin: 20px 0;
            text-align: center;
            font-family: monospace;
            font-size: 1.1em;
            line-height: 1.4;
            overflow-x: auto;
        }

        .table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }

        .table th {
            background: #667eea;
            color: white;
            padding: 15px;
            text-align: left;
        }

        .table td {
            padding: 12px 15px;
            border-bottom: 2px solid #ddd;
        }

        .grid-2 {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin: 20px 0;
        }

        .grid-3 {
            display: grid;
            grid-template-columns: 1fr 1fr 1fr;
            gap: 20px;
            margin: 20px 0;
        }

        .visual-item {
            background: #f5f5f5;
            padding: 20px;
            border-radius: 10px;
            border: 2px solid #667eea;
            text-align: center;
        }

        .visual-item strong {
            color: #667eea;
            display: block;
            margin-bottom: 15px;
            font-size: 1.2em;
        }

        .warning-box {
            background: linear-gradient(135deg, #fff3e0, #ffe0b2);
            border: 3px solid #ff9800;
            border-radius: 12px;
            padding: 25px;
            margin: 25px 0;
        }

        .warning-box h4 {
            color: #e65100;
            margin-bottom: 15px;
            font-size: 1.2em;
        }

        .toc {
            background: #f8f9fa;
            border-radius: 12px;
            padding: 25px;
            margin: 25px 0;
        }

        .toc h4 {
            color: #667eea;
            margin-bottom: 15px;
            font-size: 1.3em;
        }

        .toc ul {
            columns: 2;
            list-style: none;
            padding-left: 0;
        }

        .toc li {
            padding: 8px 0;
            border-bottom: 1px dashed #ddd;
        }

        .toc li a {
            color: #333;
            text-decoration: none;
        }

        .toc li a:hover {
            color: #667eea;
        }

        @media (max-width: 768px) {

            .grid-2,
            .grid-3 {
                grid-template-columns: 1fr;
            }

            .toc ul {
                columns: 1;
            }
        }
    </style>
</head>

<body>
    <div class="container">
        <h1>üöÄ Module 01: Foundations of Prompt Engineering</h1>

        <!-- Table of Contents -->
        <div class="section">
            <h2>üìã Core Pillars (Foundational Concepts)</h2>
            <div class="toc">
                <h4>Quick Navigation</h4>
                <ul>
                    <li>1. What are LLMs?</li>
                    <li>1b. Deep Dive: LLM Internals</li>
                    <li>2. Tokens: The Currency of LLMs</li>
                    <li>3. Context Windows (2026 Standards)</li>
                    <li>4. Temperature: Controlling Creativity</li>
                    <li>4b. Advanced Sampling Controls</li>
                    <li>5. The Prompt Engineering Mindset</li>
                    <li>6. CRISP: The Professional Framework</li>
                    <li>7. Single-Turn vs Multi-Turn</li>
                    <li>8. Common Pitfalls & Templates</li>
                </ul>
            </div>
        </div>

        <!-- Topic 1: What are LLMs? -->
        <div class="section" id="topic-1">
            <h2>1Ô∏è‚É£ What are Large Language Models (LLMs)?</h2>
            <div class="explanation">
                <strong>Core Idea:</strong> LLMs are <strong>prediction engines</strong> trained on massive text
                datasets. They don't "think" in the human sense; they predict the most probable next token based on
                statistical patterns.
            </div>
            <div class="diagram">
                <pre>
[DATASET] ‚Üí [TRAINING] ‚Üí [LLM WEIGHTS]
    ‚Üë            ‚îÇ             ‚îÇ
(Internet,    (Compute-      (Encoded
 Books, Code)  Intensive)     Knowledge)
                ‚Üì
[INPUT PROMPT] ‚Üí [INFERENCE] ‚Üí [GENERATED TEXT]
                </pre>
            </div>
            <div class="warning-box">
                <h4>‚ö†Ô∏è The Hallucination Risk</h4>
                LLMs are <strong>probabilistic</strong>, not factual. They can confidently generate false information
                ("Hallucinations") because their goal is to predict the next <em>plausible</em> word, not necessarily
                the <em>correct</em> one.
            </div>
            <div class="grid-2">
                <div class="visual-item">
                    <strong>üß† Parameters</strong>
                    Billions of numerical values encoding knowledge. (e.g., GPT-4 has ~1.76 Trillion).
                </div>
                <div class="visual-item">
                    <strong>‚ö° Inference</strong>
                    The process of using the trained model to generate a response in real-time.
                </div>
            </div>
        </div>

        <!-- Topic 1b: LLM Internals -->
        <div class="section" id="topic-1b">
            <h2>1Ô∏è‚É£b Deep Dive: LLM Internals</h2>
            <div class="explanation">
                <strong>Core Idea:</strong> Understanding the "Engine Room" helps you write better prompts. It's not
                magic; it's math.
            </div>
            <div class="grid-2">
                <div class="visual-item">
                    <strong>üßÆ Embeddings vs Tokens</strong>
                    <br>
                    <strong>Tokens</strong> are the inputs (words/parts).
                    <br>
                    <strong>Embeddings</strong> are the "meaning" (vectors).
                    <br>
                    <em>Analogy:</em> Token = The word "King". Embedding = [0.9 Royal, 0.8 Male, 0.1 Food].
                </div>
                <div class="visual-item">
                    <strong>üëÄ Attention Mechanism</strong>
                    <br>
                    How the model "focuses". When generating the word "bank", it looks back at "river" (nature) or
                    "money" (finance) to decide context.
                </div>
            </div>
            <div class="key-concept">
                <strong>üí° Positional Encoding:</strong>
                LLMs don't read left-to-right like humans. They see all tokens at once. "Positional Encodings" tell them
                that "Dog bites Man" is different from "Man bites Dog".
                <br><em>Prompt Tip:</em> Ordering matters! Put key instructions at the <strong>start</strong> or
                <strong>end</strong> (recency bias) because attention is highest there.
            </div>
            <table class="table">
                <tr>
                    <th>Training Stage</th>
                    <th>What Happens</th>
                    <th>Impact on Prompting</th>
                </tr>
                <tr>
                    <td><strong>Pre-Training</strong></td>
                    <td>Learns grammar & facts from internet.</td>
                    <td>Base knowledge (Wikipedia, Code).</td>
                </tr>
                <tr>
                    <td><strong>Instruction Tuning</strong></td>
                    <td>Learns to follow commands ("Summarize this").</td>
                    <td>Why "Chat" models follow orders better than base models.</td>
                </tr>
                <tr>
                    <td><strong>RLHF</strong></td>
                    <td>Humans rank answers (Good/Bad).</td>
                    <td>Why models refuse "harmful" prompts (Safety).</td>
                </tr>
            </table>
        </div>

        <!-- Topic 2: Tokens and Tokenization -->
        <div class="section" id="topic-2">
            <h2>2Ô∏è‚É£ Tokens: The Currency of LLMs</h2>
            <div class="explanation">
                <strong>Core Idea:</strong> LLMs process text in "tokens" (chunks of characters), not words. 1 token ‚âà
                0.75 words or 4 characters in English.
            </div>
            <div class="diagram">
                <pre>
RAW TEXT: "Tokenization"
         ‚Üì
TOKENS: ["Token", "ization"]
         ‚Üì
IDs:    [30561, 1632]
                </pre>
            </div>
            <div class="why-matters">
                <strong>üíé Why Tokens Matter:</strong><br>
                ‚Ä¢ <strong>Cost:</strong> APIs charge per token (Input + Output).<br>
                ‚Ä¢ <strong>Limits:</strong> Every model has a hard "Context Window" limit.<br>
                ‚Ä¢ <strong>Bias:</strong> Non-English languages often use more tokens per word.
            </div>
        </div>

        <!-- Topic 3: Context Windows -->
        <div class="section" id="topic-3">
            <h2>3Ô∏è‚É£ Context Windows (2026 Standards)</h2>
            <div class="explanation">
                <strong>Core Idea:</strong> The maximum capacity of information an LLM can "hold in its head" at once.
            </div>
            <table class="table">
                <tr>
                    <th>Model Family</th>
                    <th>Context Limit</th>
                    <th>Best Use</th>
                </tr>
                <tr>
                    <td><strong>Gemini 1.5 Pro</strong></td>
                    <td>2,000,000+ Tokens</td>
                    <td>Analyzing whole codebases or movies</td>
                </tr>
                <tr>
                    <td><strong>Claude 3.5 Sonnet</strong></td>
                    <td>200,000 Tokens</td>
                    <td>High-accuracy reasoning / Long docs</td>
                </tr>
                <tr>
                    <td><strong>GPT-4o / o1</strong></td>
                    <td>128,000 Tokens</td>
                    <td>Agentic tasks / General purpose</td>
                </tr>
                <tr>
                    <td><strong>Llama 3.3</strong></td>
                    <td>128,000 Tokens</td>
                    <td>Local hosting / Open-source power</td>
                </tr>
            </table>
        </div>

        <!-- Topic 4: Temperature & Sampling -->
        <div class="section" id="topic-4">
            <h2>4Ô∏è‚É£ Temperature: Controlling Creativity</h2>
            <div class="explanation">
                <strong>Core Idea:</strong> Temperature (0.0 to 2.0) controls the randomness of the model's output.
            </div>
            <div class="grid-2">
                <div class="visual-item" style="border-color: #2196f3;">
                    <strong>‚ùÑÔ∏è Low Temp (0.0 - 0.3)</strong>
                    Deterministic, focused, repetitive.<br>
                    <em>Use for: Math, Code, Facts.</em>
                </div>
                <div class="visual-item" style="border-color: #ff9800;">
                    <strong>üî• High Temp (0.7 - 1.2)</strong>
                    Creative, diverse, unpredictable.<br>
                    <em>Use for: Writing, Ideas, Brainstorming.</em>
                </div>
            </div>
            <div class="key-concept">
                <strong>üí° Pro Tip: Top-P (Nucleus Sampling)</strong><br>
                Similar to Temperature, Top-P limits the model's choices to a percentage of the most likely words.
                <strong>Top-P 0.9</strong> means the model only considers words that make up the top 90% of probability.
                Use either Temp or Top-P, rarely both.
            </div>
            <div class="key-concept">
                <strong>üí° Key Insight:</strong> For production automation (SDET), always use <strong>Temp 0</strong> to
                ensure stable, reproducible test results.
            </div>
        </div>

        <!-- Topic 4b: Advanced Parameters -->
        <div class="section" id="topic-4b">
            <h2>4Ô∏è‚É£b Advanced Sampling Controls</h2>
            <div class="explanation">
                <strong>Core Idea:</strong> Beyond Temperature, these leverage the probability distribution directly.
            </div>
            <table class="table">
                <tr>
                    <th>Parameter</th>
                    <th>Function</th>
                    <th>Use Case</th>
                </tr>
                <tr>
                    <td><strong>Top-k</strong></td>
                    <td>Cuts off the "tail" of unlikely words. "Only look at the top 50 choices."</td>
                    <td>Preventing complete nonsense in low-confidence topics.</td>
                </tr>
                <tr>
                    <td><strong>Logit Bias</strong></td>
                    <td>Manually +/- the probability of specific tokens.</td>
                    <td>Forcing the model to say "Yes" or banning the word "Happy".</td>
                </tr>
                <tr>
                    <td><strong>Stop Sequences</strong></td>
                    <td>Strings that force the model to stop generating immediately.</td>
                    <td>Stopping a chat bot when it tries to write "User:".</td>
                </tr>
                <tr>
                    <td><strong>Seed</strong></td>
                    <td>Forces deterministic output (like random.seed in Python).</td>
                    <td>Unit testing prompts to ensure 100% consistent results.</td>
                </tr>
            </table>
        </div>

        <!-- Topic 5: Mindset -->
        <div class="section" id="topic-5">
            <h2>5Ô∏è‚É£ The Prompt Engineering Mindset</h2>
            <div class="explanation">
                Prompt engineering is the iterative practice of designing inputs that guide LLMs toward specific,
                high-quality outputs.
            </div>
            <div class="grid-2">
                <div class="example-box">
                    <h4>‚ùå Vague Prompt</h4>
                    "Summarize this code."<br>
                    <em>Result: High level, generic.</em>
                </div>
                <div class="example-box">
                    <h4>‚úÖ Specific Prompt</h4>
                    "Provide a technical summary of this Python script in 3 bullet points. Focus on: 1. Main logic, 2.
                    External dependencies, 3. Potential security risks."<br>
                    <em>Result: Actionable, precise.</em>
                </div>
            </div>
            <div class="why-matters">
                <strong>The 4 Golden Rules:</strong><br>
                1. <strong>Be Specific:</strong> Clear instructions = clear results.<br>
                2. <strong>Provide Context:</strong> Tell the model WHO you are and WHAT you need.<br>
                3. <strong>Set Expectations:</strong> Define output format (JSON, Table, Markdown).<br>
                4. <strong>Iterate:</strong> Test, fail, refine, repeat.
            </div>
            <div class="key-concept">
                <strong>üí° Recency Bias:</strong> LLMs often remember instructions at the <strong>very
                    beginning</strong> and <strong>very end</strong> of a prompt better than the middle. For long
                prompts, repeat your core constraint at the bottom.
            </div>
        </div>

        <!-- Topic 6: CRISP Framework -->
        <div class="section" id="topic-6">
            <h2>6Ô∏è‚É£ CRISP: The Professional Framework</h2>
            <div class="explanation">
                <strong>Core Idea:</strong> A structured way to build any complex prompt.
            </div>
            <table class="table">
                <tr>
                    <th>Component</th>
                    <th>Meaning</th>
                    <th>Example</th>
                </tr>
                <tr>
                    <td><strong>C</strong>ontext</td>
                    <td>Background Info</td>
                    <td>"I am testing a login API..."</td>
                </tr>
                <tr>
                    <td><strong>R</strong>ole</td>
                    <td>Persona</td>
                    <td>"Act as a Senior SDET..."</td>
                </tr>
                <tr>
                    <td><strong>I</strong>ntent</td>
                    <td>Primary Goal</td>
                    <td>"Generate positive test cases..."</td>
                </tr>
                <tr>
                    <td><strong>S</strong>pecifics</td>
                    <td>Requirements</td>
                    <td>"Use Java and RestAssured..."</td>
                </tr>
                <tr>
                    <td><strong>P</strong>resentation</td>
                    <td>Format</td>
                    <td>"Output as a Markdown table..."</td>
                </tr>
            </table>
        </div>

        <!-- Topic 7: Single vs Multi-Turn -->
        <div class="section" id="topic-7">
            <h2>7Ô∏è‚É£ Single-Turn vs Multi-Turn</h2>
            <div class="explanation">
                <strong>Core Idea:</strong> Matching the interaction style to the task complexity.
            </div>
            <div class="diagram">
                <pre>
[SINGLE-TURN]  User ‚Üí Prompt ‚Üí Response (Done)
               *Fast, stateless, direct.

[MULTI-TURN]   User ‚Üí P1 ‚Üí R1 ‚Üí User ‚Üí P2 ‚Üí R2
               *Stateful, iterative, complex.
                </pre>
            </div>
            <div class="key-concept">
                <strong>üí° Key Insight:</strong> Multi-turn is essentially "System Memory". In modern apps, we use
                <strong>Thread IDs</strong> to manage this state.
            </div>
        </div>

        <!-- Topic 8: Pitfalls & Templates -->
        <div class="section" id="topic-8">
            <h2>8Ô∏è‚É£ Common Pitfalls & Templates</h2>
            <div class="warning-box">
                <h4>‚ö†Ô∏è Avoid These Mistakes:</h4>
                ‚Ä¢ <strong>Ambiguity:</strong> Saying "Make it good" instead of "Increase readability".<br>
                ‚Ä¢ <strong>Overloading:</strong> Asking 5 questions in one paragraph.<br>
                ‚Ä¢ <strong>Ignoring Edge Cases:</strong> Not telling the model what to do if data is missing.
            </div>
            <div class="diagram">
                <pre>
[PROMPT TEMPLATE]
# ROLE: [Senior AI Engineer]
# CONTEXT: [Working on RAG System]
# TASK: [Optimize Embedding Retrieval]
# CONSTRAINTS: [Max 500 tokens, JSON only]
                </pre>
            </div>
        </div>

        <!-- Summary Section -->
        <div class="section">
            <h2>üìö Quick Reference Summary</h2>
            <table class="table">
                <tr>
                    <th>#</th>
                    <th>Foundational Concept</th>
                    <th>Key takeaway</th>
                </tr>
                <tr>
                    <td>1</td>
                    <td><strong>LLM Definition</strong></td>
                    <td>Statistical next-token predictors.</td>
                </tr>
                <tr>
                    <td>2</td>
                    <td><strong>Tokens</strong></td>
                    <td>The atomic units of LLM cost and memory.</td>
                </tr>
                <tr>
                    <td>3</td>
                    <td><strong>Context Window</strong></td>
                    <td>The 'Working Memory' of the model.</td>
                </tr>
                <tr>
                    <td>4</td>
                    <td><strong>Temperature</strong></td>
                    <td>The slider for choosing consistency vs creativity.</td>
                </tr>
                <tr>
                    <td>5</td>
                    <td><strong>Specificity</strong></td>
                    <td>The #1 way to improve output quality.</td>
                </tr>
                <tr>
                    <td>6</td>
                    <td><strong>CRISP</strong></td>
                    <td>The framework for structured prompting.</td>
                </tr>
                <tr>
                    <td>7</td>
                    <td><strong>Multi-Turn</strong></td>
                    <td>Best for complex, multi-step reasoning.</td>
                </tr>
            </table>
            <div class="summary-box">
                üí° Mastering these foundations is the bridge between using AI as a toy and using it as a
                production-grade tool.
            </div>
        </div>

    </div>
</body>

</html>